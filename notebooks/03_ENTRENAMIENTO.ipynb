{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c8ff48",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cea8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_device = pd.DataFrame({\n",
    "    'Componente': ['Device', 'CUDA Disponible', 'Nombre GPU'],\n",
    "    'Valor': [\n",
    "        str(device),\n",
    "        torch.cuda.is_available(),\n",
    "        torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "    ]\n",
    "})\n",
    "\n",
    "df_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae642b",
   "metadata": {},
   "source": [
    "## 2. Configuración de Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_base = Path(r'e:\\06. Sexto Ciclo\\01. Machine Learning\\07. Workspace\\16S03. Proyecto 03\\P3-EcoSort')\n",
    "ruta_features = ruta_base / 'result' / 'features'\n",
    "ruta_modelos = ruta_base / 'result' / 'models'\n",
    "ruta_figuras = ruta_base / 'result' / 'figures'\n",
    "\n",
    "ruta_modelos.mkdir(parents=True, exist_ok=True)\n",
    "ruta_figuras.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clases = ['general', 'paper', 'plastic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd808bad",
   "metadata": {},
   "source": [
    "## 3. Carga de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img = np.load(ruta_features / 'X_train_imagenes.npy')\n",
    "y_train = np.load(ruta_features / 'y_train.npy')\n",
    "X_val_img = np.load(ruta_features / 'X_val_imagenes.npy')\n",
    "y_val = np.load(ruta_features / 'y_val.npy')\n",
    "\n",
    "X_train_pca = np.load(ruta_features / 'features_train_pca.npy')\n",
    "X_val_pca = np.load(ruta_features / 'features_val_pca.npy')\n",
    "\n",
    "df_carga = pd.DataFrame({\n",
    "    'Dataset': ['Train Imágenes', 'Val Imágenes', 'Train PCA', 'Val PCA'],\n",
    "    'Shape': [X_train_img.shape, X_val_img.shape, X_train_pca.shape, X_val_pca.shape],\n",
    "    'Etiquetas': [len(y_train), len(y_val), len(y_train), len(y_val)]\n",
    "})\n",
    "\n",
    "df_carga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a653e",
   "metadata": {},
   "source": [
    "## 4. MODELO 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c46365",
   "metadata": {},
   "source": [
    "### 4.1 Optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b864e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "modelo_lr_base = LogisticRegression(random_state=42)\n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    modelo_lr_base, \n",
    "    param_grid_lr, \n",
    "    cv=5, \n",
    "    scoring='f1_macro', \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train_pca, y_train)\n",
    "\n",
    "mejores_params_lr = grid_search_lr.best_params_\n",
    "mejor_score_lr = grid_search_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_params = pd.DataFrame({\n",
    "    'Parámetro': list(mejores_params_lr.keys()) + ['F1 Macro CV'],\n",
    "    'Valor': list(mejores_params_lr.values()) + [mejor_score_lr]\n",
    "})\n",
    "\n",
    "df_lr_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d12f7a",
   "metadata": {},
   "source": [
    "### 4.2 Entrenamiento Final y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lr_final = grid_search_lr.best_estimator_\n",
    "\n",
    "y_pred_train_lr = modelo_lr_final.predict(X_train_pca)\n",
    "y_pred_val_lr = modelo_lr_final.predict(X_val_pca)\n",
    "\n",
    "f1_macro_train_lr = f1_score(y_train, y_pred_train_lr, average='macro')\n",
    "f1_micro_train_lr = f1_score(y_train, y_pred_train_lr, average='micro')\n",
    "f1_macro_val_lr = f1_score(y_val, y_pred_val_lr, average='macro')\n",
    "f1_micro_val_lr = f1_score(y_val, y_pred_val_lr, average='micro')\n",
    "\n",
    "y_train_bin = label_binarize(y_train, classes=[0, 1, 2])\n",
    "y_val_bin = label_binarize(y_val, classes=[0, 1, 2])\n",
    "y_pred_proba_lr = modelo_lr_final.predict_proba(X_val_pca)\n",
    "\n",
    "precision_lr = {}\n",
    "recall_lr = {}\n",
    "auc_pr_lr = {}\n",
    "\n",
    "for i in range(3):\n",
    "    precision_lr[i], recall_lr[i], _ = precision_recall_curve(y_val_bin[:, i], y_pred_proba_lr[:, i])\n",
    "    auc_pr_lr[i] = auc(recall_lr[i], precision_lr[i])\n",
    "\n",
    "auc_pr_macro_lr = np.mean(list(auc_pr_lr.values()))\n",
    "\n",
    "df_lr_metricas = pd.DataFrame({\n",
    "    'Métrica': ['F1 Macro Train', 'F1 Micro Train', 'F1 Macro Val', 'F1 Micro Val', 'AUC-PR Macro Val'],\n",
    "    'Valor': [f1_macro_train_lr, f1_micro_train_lr, f1_macro_val_lr, f1_micro_val_lr, auc_pr_macro_lr]\n",
    "})\n",
    "\n",
    "df_lr_metricas['Valor'] = df_lr_metricas['Valor'].round(4)\n",
    "df_lr_metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb048b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_lr = confusion_matrix(y_val, y_pred_val_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=clases, yticklabels=clases, \n",
    "           cbar_kws={'label': 'Cantidad'}, linewidths=2, linecolor='black', ax=ax)\n",
    "ax.set_xlabel('Predicción', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Valor Real', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Matriz de Confusión - Logistic Regression\\nF1 Macro: {f1_macro_val_lr:.4f}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ruta_figuras / '03_train_01_lr_confusion.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd977404",
   "metadata": {},
   "source": [
    "### 4.3 Guardar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49772433",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(modelo_lr_final, ruta_modelos / 'logistic_regression.pkl')\n",
    "joblib.dump(mejores_params_lr, ruta_modelos / 'logistic_regression_params.pkl')\n",
    "\n",
    "metricas_lr = {\n",
    "    'f1_macro_train': f1_macro_train_lr,\n",
    "    'f1_micro_train': f1_micro_train_lr,\n",
    "    'f1_macro_val': f1_macro_val_lr,\n",
    "    'f1_micro_val': f1_micro_val_lr,\n",
    "    'auc_pr_macro_val': auc_pr_macro_lr\n",
    "}\n",
    "joblib.dump(metricas_lr, ruta_modelos / 'logistic_regression_metricas.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb586b",
   "metadata": {},
   "source": [
    "## 5. MODELO 2: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6cbe9",
   "metadata": {},
   "source": [
    "### 5.1 Optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "modelo_svm_base = SVC(random_state=42, probability=True)\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    modelo_svm_base,\n",
    "    param_distributions_svm,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search_svm.fit(X_train_pca, y_train)\n",
    "\n",
    "mejores_params_svm = random_search_svm.best_params_\n",
    "mejor_score_svm = random_search_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d771ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm_params = pd.DataFrame({\n",
    "    'Parámetro': list(mejores_params_svm.keys()) + ['F1 Macro CV'],\n",
    "    'Valor': [str(v) for v in mejores_params_svm.values()] + [mejor_score_svm]\n",
    "})\n",
    "\n",
    "df_svm_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e84d50",
   "metadata": {},
   "source": [
    "### 5.2 Entrenamiento Final y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4463d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_svm_final = random_search_svm.best_estimator_\n",
    "\n",
    "y_pred_train_svm = modelo_svm_final.predict(X_train_pca)\n",
    "y_pred_val_svm = modelo_svm_final.predict(X_val_pca)\n",
    "\n",
    "f1_macro_train_svm = f1_score(y_train, y_pred_train_svm, average='macro')\n",
    "f1_micro_train_svm = f1_score(y_train, y_pred_train_svm, average='micro')\n",
    "f1_macro_val_svm = f1_score(y_val, y_pred_val_svm, average='macro')\n",
    "f1_micro_val_svm = f1_score(y_val, y_pred_val_svm, average='micro')\n",
    "\n",
    "y_pred_proba_svm = modelo_svm_final.predict_proba(X_val_pca)\n",
    "\n",
    "precision_svm = {}\n",
    "recall_svm = {}\n",
    "auc_pr_svm = {}\n",
    "\n",
    "for i in range(3):\n",
    "    precision_svm[i], recall_svm[i], _ = precision_recall_curve(y_val_bin[:, i], y_pred_proba_svm[:, i])\n",
    "    auc_pr_svm[i] = auc(recall_svm[i], precision_svm[i])\n",
    "\n",
    "auc_pr_macro_svm = np.mean(list(auc_pr_svm.values()))\n",
    "\n",
    "df_svm_metricas = pd.DataFrame({\n",
    "    'Métrica': ['F1 Macro Train', 'F1 Micro Train', 'F1 Macro Val', 'F1 Micro Val', 'AUC-PR Macro Val'],\n",
    "    'Valor': [f1_macro_train_svm, f1_micro_train_svm, f1_macro_val_svm, f1_micro_val_svm, auc_pr_macro_svm]\n",
    "})\n",
    "\n",
    "df_svm_metricas['Valor'] = df_svm_metricas['Valor'].round(4)\n",
    "df_svm_metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f68b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_svm = confusion_matrix(y_val, y_pred_val_svm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Greens', xticklabels=clases, yticklabels=clases, \n",
    "           cbar_kws={'label': 'Cantidad'}, linewidths=2, linecolor='black', ax=ax)\n",
    "ax.set_xlabel('Predicción', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Valor Real', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Matriz de Confusión - SVM\\nF1 Macro: {f1_macro_val_svm:.4f}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ruta_figuras / '03_train_02_svm_confusion.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf5798",
   "metadata": {},
   "source": [
    "### 5.3 Guardar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(modelo_svm_final, ruta_modelos / 'svm.pkl')\n",
    "joblib.dump(mejores_params_svm, ruta_modelos / 'svm_params.pkl')\n",
    "\n",
    "metricas_svm = {\n",
    "    'f1_macro_train': f1_macro_train_svm,\n",
    "    'f1_micro_train': f1_micro_train_svm,\n",
    "    'f1_macro_val': f1_macro_val_svm,\n",
    "    'f1_micro_val': f1_micro_val_svm,\n",
    "    'auc_pr_macro_val': auc_pr_macro_svm\n",
    "}\n",
    "joblib.dump(metricas_svm, ruta_modelos / 'svm_metricas.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251aabd",
   "metadata": {},
   "source": [
    "## 6. MODELO 3: CNN con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7416b",
   "metadata": {},
   "source": [
    "### 6.1 Definición de Arquitectura CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e09e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Clasificador(nn.Module):\n",
    "    def __init__(self, num_clases=3, dropout=0.5):\n",
    "        super(CNN_Clasificador, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(128, num_clases)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697c2a5",
   "metadata": {},
   "source": [
    "### 6.2 Preparación de Datos para PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12350be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train_img).permute(0, 3, 1, 2)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val_img).permute(0, 3, 1, 2)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f2c1b",
   "metadata": {},
   "source": [
    "### 6.3 Función de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_cnn(modelo, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    historial_train_loss = []\n",
    "    historial_val_loss = []\n",
    "    historial_train_acc = []\n",
    "    historial_val_acc = []\n",
    "    \n",
    "    mejor_val_loss = float('inf')\n",
    "    mejor_modelo = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        modelo.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Train'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = modelo(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        modelo.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = modelo(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        historial_train_loss.append(train_loss)\n",
    "        historial_val_loss.append(val_loss)\n",
    "        historial_train_acc.append(train_acc)\n",
    "        historial_val_acc.append(val_acc)\n",
    "        \n",
    "        tqdm.write(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, '\n",
    "                  f'Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
    "        \n",
    "        if val_loss < mejor_val_loss:\n",
    "            mejor_val_loss = val_loss\n",
    "            mejor_modelo = modelo.state_dict().copy()\n",
    "    \n",
    "    modelo.load_state_dict(mejor_modelo)\n",
    "    \n",
    "    return modelo, historial_train_loss, historial_val_loss, historial_train_acc, historial_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e4559",
   "metadata": {},
   "source": [
    "### 6.4 Entrenamiento del Modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6782407",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cnn = CNN_Clasificador(num_clases=3, dropout=0.5).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelo_cnn.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "modelo_cnn, train_loss_cnn, val_loss_cnn, train_acc_cnn, val_acc_cnn = entrenar_cnn(\n",
    "    modelo_cnn, train_loader, val_loader, criterion, optimizer, num_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c1187",
   "metadata": {},
   "source": [
    "### 6.5 Visualización de Curvas de Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "epochs = range(1, len(train_loss_cnn) + 1)\n",
    "\n",
    "axes[0].plot(epochs, train_loss_cnn, 'b-', linewidth=2, label='Train Loss', marker='o', markersize=4)\n",
    "axes[0].plot(epochs, val_loss_cnn, 'r-', linewidth=2, label='Validation Loss', marker='s', markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Curvas de Pérdida - CNN', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs, train_acc_cnn, 'b-', linewidth=2, label='Train Accuracy', marker='o', markersize=4)\n",
    "axes[1].plot(epochs, val_acc_cnn, 'r-', linewidth=2, label='Validation Accuracy', marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Curvas de Accuracy - CNN', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ruta_figuras / '03_train_03_cnn_curvas.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db8d51",
   "metadata": {},
   "source": [
    "### 6.6 Evaluación del Modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo_pytorch(modelo, data_loader):\n",
    "    modelo.eval()\n",
    "    todas_predicciones = []\n",
    "    todas_etiquetas = []\n",
    "    todas_probabilidades = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = modelo(inputs)\n",
    "            probabilidades = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            todas_predicciones.extend(predicted.cpu().numpy())\n",
    "            todas_etiquetas.extend(labels.numpy())\n",
    "            todas_probabilidades.extend(probabilidades.cpu().numpy())\n",
    "    \n",
    "    return np.array(todas_predicciones), np.array(todas_etiquetas), np.array(todas_probabilidades)\n",
    "\n",
    "y_pred_train_cnn, y_true_train_cnn, y_proba_train_cnn = evaluar_modelo_pytorch(modelo_cnn, train_loader)\n",
    "y_pred_val_cnn, y_true_val_cnn, y_proba_val_cnn = evaluar_modelo_pytorch(modelo_cnn, val_loader)\n",
    "\n",
    "f1_macro_train_cnn = f1_score(y_true_train_cnn, y_pred_train_cnn, average='macro')\n",
    "f1_micro_train_cnn = f1_score(y_true_train_cnn, y_pred_train_cnn, average='micro')\n",
    "f1_macro_val_cnn = f1_score(y_true_val_cnn, y_pred_val_cnn, average='macro')\n",
    "f1_micro_val_cnn = f1_score(y_true_val_cnn, y_pred_val_cnn, average='micro')\n",
    "\n",
    "y_true_val_bin_cnn = label_binarize(y_true_val_cnn, classes=[0, 1, 2])\n",
    "\n",
    "precision_cnn = {}\n",
    "recall_cnn = {}\n",
    "auc_pr_cnn = {}\n",
    "\n",
    "for i in range(3):\n",
    "    precision_cnn[i], recall_cnn[i], _ = precision_recall_curve(y_true_val_bin_cnn[:, i], y_proba_val_cnn[:, i])\n",
    "    auc_pr_cnn[i] = auc(recall_cnn[i], precision_cnn[i])\n",
    "\n",
    "auc_pr_macro_cnn = np.mean(list(auc_pr_cnn.values()))\n",
    "\n",
    "df_cnn_metricas = pd.DataFrame({\n",
    "    'Métrica': ['F1 Macro Train', 'F1 Micro Train', 'F1 Macro Val', 'F1 Micro Val', 'AUC-PR Macro Val'],\n",
    "    'Valor': [f1_macro_train_cnn, f1_micro_train_cnn, f1_macro_val_cnn, f1_micro_val_cnn, auc_pr_macro_cnn]\n",
    "})\n",
    "\n",
    "df_cnn_metricas['Valor'] = df_cnn_metricas['Valor'].round(4)\n",
    "df_cnn_metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cnn = confusion_matrix(y_true_val_cnn, y_pred_val_cnn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Reds', xticklabels=clases, yticklabels=clases, \n",
    "           cbar_kws={'label': 'Cantidad'}, linewidths=2, linecolor='black', ax=ax)\n",
    "ax.set_xlabel('Predicción', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Valor Real', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Matriz de Confusión - CNN\\nF1 Macro: {f1_macro_val_cnn:.4f}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ruta_figuras / '03_train_04_cnn_confusion.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4eda9",
   "metadata": {},
   "source": [
    "### 6.7 Guardar Modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3dfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelo_cnn.state_dict(), ruta_modelos / 'cnn_model.pth')\n",
    "\n",
    "historial_cnn = {\n",
    "    'train_loss': train_loss_cnn,\n",
    "    'val_loss': val_loss_cnn,\n",
    "    'train_acc': train_acc_cnn,\n",
    "    'val_acc': val_acc_cnn\n",
    "}\n",
    "joblib.dump(historial_cnn, ruta_modelos / 'cnn_historial.pkl')\n",
    "\n",
    "metricas_cnn = {\n",
    "    'f1_macro_train': f1_macro_train_cnn,\n",
    "    'f1_micro_train': f1_micro_train_cnn,\n",
    "    'f1_macro_val': f1_macro_val_cnn,\n",
    "    'f1_micro_val': f1_micro_val_cnn,\n",
    "    'auc_pr_macro_val': auc_pr_macro_cnn\n",
    "}\n",
    "joblib.dump(metricas_cnn, ruta_modelos / 'cnn_metricas.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70706eff",
   "metadata": {},
   "source": [
    "## 7. Resumen de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de01d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resumen = pd.DataFrame({\n",
    "    'Modelo': ['Logistic Regression', 'SVM', 'CNN'],\n",
    "    'F1 Macro Train': [f1_macro_train_lr, f1_macro_train_svm, f1_macro_train_cnn],\n",
    "    'F1 Macro Val': [f1_macro_val_lr, f1_macro_val_svm, f1_macro_val_cnn],\n",
    "    'F1 Micro Val': [f1_micro_val_lr, f1_micro_val_svm, f1_micro_val_cnn],\n",
    "    'AUC-PR Val': [auc_pr_macro_lr, auc_pr_macro_svm, auc_pr_macro_cnn]\n",
    "})\n",
    "\n",
    "df_resumen = df_resumen.round(4)\n",
    "df_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e867c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "modelos = df_resumen['Modelo']\n",
    "colores_modelos = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "axes[0].bar(modelos, df_resumen['F1 Macro Val'], alpha=0.8, color=colores_modelos, edgecolor='black')\n",
    "axes[0].set_ylabel('F1 Macro Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('F1 Macro - Validación', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_resumen['F1 Macro Val']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "axes[1].bar(modelos, df_resumen['F1 Micro Val'], alpha=0.8, color=colores_modelos, edgecolor='black')\n",
    "axes[1].set_ylabel('F1 Micro Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('F1 Micro - Validación', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_resumen['F1 Micro Val']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "axes[2].bar(modelos, df_resumen['AUC-PR Val'], alpha=0.8, color=colores_modelos, edgecolor='black')\n",
    "axes[2].set_ylabel('AUC-PR Score', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('AUC-PR Macro - Validación', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_resumen['AUC-PR Val']):\n",
    "    axes[2].text(i, v + 0.01, f'{v:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ruta_figuras / '03_train_05_comparacion_modelos.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3d6bb",
   "metadata": {},
   "source": [
    "### Conclusiones:\n",
    "\n",
    "**1. Modelos Entrenados:**\n",
    "- Logistic Regression: Modelo base simple con optimización de hiperparámetros\n",
    "- SVM: Modelo avanzado con kernel optimizado mediante Random Search\n",
    "- CNN: Red neuronal convolucional profunda entrenada con PyTorch/CUDA\n",
    "\n",
    "**2. Optimización:**\n",
    "- Cada modelo fue optimizado exhaustivamente variando todos sus hiperparámetros\n",
    "- Cross-validation utilizado para validar hiperparámetros\n",
    "- Métricas orientadas al problema: F1 Macro como objetivo principal\n",
    "\n",
    "**3. Curvas de Pérdida:**\n",
    "- CNN muestra convergencia clara en las curvas train vs validation\n",
    "- Todas las curvas guardadas para análisis posterior\n",
    "\n",
    "**4. Artefactos Guardados:**\n",
    "- Modelos entrenados guardados en result/models/\n",
    "- Hiperparámetros óptimos y métricas almacenadas\n",
    "- Curvas de pérdida guardadas para evaluación posterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "06C_Machine_15S02_Proyecto03",
   "language": "python",
   "name": "06c_machine_15s02_proyecto03"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
