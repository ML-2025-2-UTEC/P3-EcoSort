{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7b039b",
   "metadata": {},
   "source": [
    "# üöÄ ENTRENAMIENTO OPTIMIZADO - Mejoras para Aumentar Scores\n",
    "\n",
    "## Mejoras implementadas:\n",
    "1. **Logistic Regression**: Algoritmos m√°s robustos, mejores hiperpar√°metros\n",
    "2. **SVM**: Kernels optimizados, class_weight balanceado\n",
    "3. **CNN**: Arquitectura mejorada, data augmentation avanzado, optimizadores modernos\n",
    "4. **Ensemble**: Combinaci√≥n de modelos para m√°ximo rendimiento\n",
    "5. **Cross-validation**: Validaci√≥n m√°s robusta\n",
    "6. **Feature engineering**: Caracter√≠sticas adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modelos cl√°sicos mejorados\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, precision_recall_curve, auc, log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Deep learning mejorado\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "ruta_base = Path('/home/zamirlm/Documents/Utec/Ciclo2025-2/ML-PROYECTOS/P3-EcoSort')\n",
    "ruta_features = ruta_base / 'result' / 'features'\n",
    "ruta_modelos = ruta_base / 'result' / 'models'\n",
    "ruta_figuras = ruta_base / 'result' / 'figures'\n",
    "\n",
    "clases = ['general', 'paper', 'plastic']\n",
    "num_clases = len(clases)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "X_train_img = np.load(ruta_features / 'X_train_imagenes.npy')\n",
    "X_val_img = np.load(ruta_features / 'X_val_imagenes.npy')\n",
    "y_train = np.load(ruta_features / 'y_train.npy')\n",
    "y_val = np.load(ruta_features / 'y_val.npy')\n",
    "\n",
    "features_train_pca = np.load(ruta_features / 'features_train_pca.npy')\n",
    "features_val_pca = np.load(ruta_features / 'features_val_pca.npy')\n",
    "\n",
    "# Caracter√≠sticas originales sin PCA para mejor rendimiento\n",
    "features_train_orig = np.load(ruta_features / 'features_train_combinadas.npy')\n",
    "features_val_orig = np.load(ruta_features / 'features_val_combinadas.npy')\n",
    "\n",
    "scaler = joblib.load(ruta_features / 'scaler.pkl')\n",
    "pca = joblib.load(ruta_features / 'pca_model.pkl')\n",
    "\n",
    "print(f\"Datos cargados:\")\n",
    "print(f\"Im√°genes train: {X_train_img.shape}, val: {X_val_img.shape}\")\n",
    "print(f\"Features PCA train: {features_train_pca.shape}, val: {features_val_pca.shape}\")\n",
    "print(f\"Features orig train: {features_train_orig.shape}, val: {features_val_orig.shape}\")\n",
    "print(f\"Distribuci√≥n clases train: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng",
   "metadata": {},
   "source": [
    "## üîß 1. FEATURE ENGINEERING MEJORADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "additional_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas_adicionales(features_orig):\n",
    "    \"\"\"Extrae caracter√≠sticas adicionales para mejorar rendimiento\"\"\"\n",
    "    \n",
    "    # Caracter√≠sticas polin√≥micas de orden 2 (solo las m√°s importantes)\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    \n",
    "    # Solo tomar subset de features m√°s importantes para polin√≥micas\n",
    "    subset_features = features_orig[:, :50]  # Primeras 50 caracter√≠sticas\n",
    "    poly_features = poly.fit_transform(subset_features)\n",
    "    \n",
    "    # Caracter√≠sticas estad√≠sticas adicionales\n",
    "    stats_features = np.column_stack([\n",
    "        np.var(features_orig, axis=1),      # Varianza\n",
    "        np.std(features_orig, axis=1),      # Desviaci√≥n est√°ndar\n",
    "        np.max(features_orig, axis=1),      # M√°ximo\n",
    "        np.min(features_orig, axis=1),      # M√≠nimo\n",
    "        np.ptp(features_orig, axis=1),      # Rango (max - min)\n",
    "        np.percentile(features_orig, 25, axis=1),  # Cuartil 1\n",
    "        np.percentile(features_orig, 75, axis=1),  # Cuartil 3\n",
    "        np.median(features_orig, axis=1),   # Mediana\n",
    "    ])\n",
    "    \n",
    "    # Combinar caracter√≠sticas\n",
    "    enhanced_features = np.column_stack([\n",
    "        features_orig,\n",
    "        poly_features[:, subset_features.shape[1]:],  # Solo t√©rminos de interacci√≥n\n",
    "        stats_features\n",
    "    ])\n",
    "    \n",
    "    return enhanced_features\n",
    "\n",
    "# Crear caracter√≠sticas mejoradas\n",
    "print(\"Creando caracter√≠sticas mejoradas...\")\n",
    "features_train_enhanced = extraer_caracteristicas_adicionales(features_train_orig)\n",
    "features_val_enhanced = extraer_caracteristicas_adicionales(features_val_orig)\n",
    "\n",
    "# Escalado de las nuevas caracter√≠sticas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_enhanced = StandardScaler()\n",
    "features_train_enhanced_scaled = scaler_enhanced.fit_transform(features_train_enhanced)\n",
    "features_val_enhanced_scaled = scaler_enhanced.transform(features_val_enhanced)\n",
    "\n",
    "print(f\"Caracter√≠sticas mejoradas - Train: {features_train_enhanced_scaled.shape}, Val: {features_val_enhanced_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class_weights",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 2. C√ÅLCULO DE PESOS DE CLASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular pesos de clase para manejar desbalance\n",
    "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"Pesos de clase calculados:\")\n",
    "for clase, peso in class_weights_dict.items():\n",
    "    print(f\"Clase {clase} ({clases[clase]}): peso {peso:.3f}\")\n",
    "\n",
    "# Para PyTorch\n",
    "class_weights_tensor = torch.FloatTensor(class_weights_array).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr_improved",
   "metadata": {},
   "source": [
    "## üéØ 3. LOGISTIC REGRESSION OPTIMIZADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_lr_optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_logistic_regression_optimizada():\n",
    "    \"\"\"Entrena Logistic Regression con mejores hiperpar√°metros y algoritmos\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Entrenando Logistic Regression Optimizada...\")\n",
    "    \n",
    "    # Grid search m√°s exhaustivo\n",
    "    param_grid_lr = {\n",
    "        'C': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0],  # M√°s valores\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga'],  # M√∫ltiples solvers\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Todas las penalizaciones\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],  # Para elasticnet\n",
    "        'max_iter': [2000, 5000]  # M√°s iteraciones\n",
    "    }\n",
    "    \n",
    "    # Validaci√≥n cruzada estratificada\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Buscar mejores hiperpar√°metros\n",
    "    lr_model = LogisticRegression(\n",
    "        class_weight='balanced',  # Manejo de desbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Usar RandomizedSearchCV para eficiencia con grid grande\n",
    "    lr_search = RandomizedSearchCV(\n",
    "        lr_model, \n",
    "        param_grid_lr, \n",
    "        cv=cv,\n",
    "        scoring='f1_macro',  # M√©trica principal\n",
    "        n_iter=100,  # M√°s iteraciones\n",
    "        n_jobs=-1, \n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entrenar con caracter√≠sticas mejoradas\n",
    "    lr_search.fit(features_train_enhanced_scaled, y_train)\n",
    "    \n",
    "    mejor_lr = lr_search.best_estimator_\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_lr = mejor_lr.predict(features_val_enhanced_scaled)\n",
    "    y_pred_proba_lr = mejor_lr.predict_proba(features_val_enhanced_scaled)\n",
    "    \n",
    "    print(f\"‚úÖ Mejores hiperpar√°metros LR: {lr_search.best_params_}\")\n",
    "    print(f\"‚úÖ Mejor score CV: {lr_search.best_score_:.4f}\")\n",
    "    \n",
    "    return mejor_lr, y_pred_lr, y_pred_proba_lr\n",
    "\n",
    "modelo_lr_opt, y_pred_lr_opt, y_pred_proba_lr_opt = entrenar_logistic_regression_optimizada()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svm_improved",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4. SVM OPTIMIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_svm_optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_svm_optimizado():\n",
    "    \"\"\"Entrena SVM con kernels optimizados y mejores hiperpar√°metros\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Entrenando SVM Optimizado...\")\n",
    "    \n",
    "    # Grid search m√°s sofisticado\n",
    "    param_distributions_svm = {\n",
    "        'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0],\n",
    "        'degree': [2, 3, 4, 5],  # Para kernel polinomial\n",
    "        'coef0': [0.0, 0.1, 0.5, 1.0]  # Para poly y sigmoid\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    svm_model = SVC(\n",
    "        class_weight='balanced',\n",
    "        probability=True,  # Para probabilidades\n",
    "        random_state=42,\n",
    "        cache_size=1000  # M√°s memoria cache\n",
    "    )\n",
    "    \n",
    "    svm_search = RandomizedSearchCV(\n",
    "        svm_model, \n",
    "        param_distributions_svm, \n",
    "        cv=cv,\n",
    "        scoring='f1_macro',\n",
    "        n_iter=80,  # M√°s iteraciones\n",
    "        n_jobs=-1, \n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Usar caracter√≠sticas PCA para SVM (m√°s eficiente)\n",
    "    svm_search.fit(features_train_pca, y_train)\n",
    "    \n",
    "    mejor_svm = svm_search.best_estimator_\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_svm = mejor_svm.predict(features_val_pca)\n",
    "    y_pred_proba_svm = mejor_svm.predict_proba(features_val_pca)\n",
    "    \n",
    "    print(f\"‚úÖ Mejores hiperpar√°metros SVM: {svm_search.best_params_}\")\n",
    "    print(f\"‚úÖ Mejor score CV: {svm_search.best_score_:.4f}\")\n",
    "    \n",
    "    return mejor_svm, y_pred_svm, y_pred_proba_svm\n",
    "\n",
    "modelo_svm_opt, y_pred_svm_opt, y_pred_proba_svm_opt = entrenar_svm_optimizado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn_improved",
   "metadata": {},
   "source": [
    "## üß† 5. CNN ARQUITECTURA MEJORADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cnn_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Mejorada(nn.Module):\n",
    "    \"\"\"CNN con arquitectura mejorada y t√©cnicas modernas\"\"\"\n",
    "    \n",
    "    def __init__(self, num_clases=3, dropout_rate=0.3):\n",
    "        super(CNN_Mejorada, self).__init__()\n",
    "        \n",
    "        # Bloque 1: Caracter√≠sticas de bajo nivel\n",
    "        self.bloque1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(dropout_rate * 0.5)\n",
    "        )\n",
    "        \n",
    "        # Bloque 2: Caracter√≠sticas intermedias\n",
    "        self.bloque2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(dropout_rate * 0.6)\n",
    "        )\n",
    "        \n",
    "        # Bloque 3: Caracter√≠sticas de alto nivel\n",
    "        self.bloque3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(dropout_rate * 0.7)\n",
    "        )\n",
    "        \n",
    "        # Bloque 4: Caracter√≠sticas muy espec√≠ficas (NUEVO)\n",
    "        self.bloque4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),  # Pool adaptativo\n",
    "            nn.Dropout2d(dropout_rate * 0.8)\n",
    "        )\n",
    "        \n",
    "        # Clasificador mejorado\n",
    "        self.clasificador = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Primera capa densa\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Segunda capa densa\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Tercera capa densa\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            \n",
    "            # Capa de salida\n",
    "            nn.Linear(128, num_clases)\n",
    "        )\n",
    "        \n",
    "        # Inicializaci√≥n de pesos Xavier\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bloque1(x)\n",
    "        x = self.bloque2(x)\n",
    "        x = self.bloque3(x)\n",
    "        x = self.bloque4(x)\n",
    "        x = self.clasificador(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Arquitectura CNN mejorada definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentationAvanzado:\n",
    "    \"\"\"Data augmentation m√°s sofisticado con t√©cnicas modernas\"\"\"\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        self.transforms = transforms.Compose([\n",
    "            # Transformaciones geom√©tricas\n",
    "            transforms.RandomRotation(degrees=(-20, 20)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            \n",
    "            # Transformaciones de color m√°s sofisticadas\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "            \n",
    "            # Normalizaci√≥n\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, img_tensor):\n",
    "        return self.transforms(img_tensor)\n",
    "\n",
    "# Crear dataset con augmentation mejorado\n",
    "class DatasetConAugmentation(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, augment=False):\n",
    "        self.images = torch.FloatTensor(images).permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.augment = augment\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = DataAugmentationAvanzado()\n",
    "        else:\n",
    "            self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ Data augmentation avanzado configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_cnn_improved",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_cnn_mejorada():\n",
    "    \"\"\"Entrena CNN con arquitectura y t√©cnicas mejoradas\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Entrenando CNN Mejorada...\")\n",
    "    \n",
    "    # Hiperpar√°metros optimizados\n",
    "    mejores_params = {\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 80,  # M√°s √©pocas\n",
    "        'dropout': 0.3,\n",
    "        'weight_decay': 0.001\n",
    "    }\n",
    "    \n",
    "    # Crear datasets\n",
    "    dataset_train = DatasetConAugmentation(X_train_img, y_train, augment=True)\n",
    "    dataset_val = DatasetConAugmentation(X_val_img, y_val, augment=False)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=mejores_params['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset_val, \n",
    "        batch_size=mejores_params['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Modelo\n",
    "    modelo = CNN_Mejorada(num_clases=num_clases, dropout_rate=mejores_params['dropout']).to(device)\n",
    "    \n",
    "    # Funci√≥n de p√©rdida con pesos de clase\n",
    "    criterio = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # Optimizador AdamW (mejor que Adam)\n",
    "    optimizador = optim.AdamW(\n",
    "        modelo.parameters(), \n",
    "        lr=mejores_params['learning_rate'],\n",
    "        weight_decay=mejores_params['weight_decay'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Scheduler de learning rate\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizador, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=8, \n",
    "        min_lr=1e-6,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping mejorado\n",
    "    mejor_val_loss = float('inf')\n",
    "    paciencia = 15\n",
    "    contador_paciencia = 0\n",
    "    mejor_modelo_state = None\n",
    "    \n",
    "    # Listas para almacenar m√©tricas\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    print(f\"Iniciando entrenamiento por {mejores_params['epochs']} √©pocas...\")\n",
    "    \n",
    "    for epoch in range(mejores_params['epochs']):\n",
    "        # Entrenamiento\n",
    "        modelo.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (datos, etiquetas) in enumerate(tqdm(train_loader, desc=f'√âpoca {epoch+1}/{mejores_params[\"epochs\"]}')):\n",
    "            datos, etiquetas = datos.to(device), etiquetas.to(device)\n",
    "            \n",
    "            optimizador.zero_grad()\n",
    "            salidas = modelo(datos)\n",
    "            loss = criterio(salidas, etiquetas)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(modelo.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizador.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(salidas.data, 1)\n",
    "            train_total += etiquetas.size(0)\n",
    "            train_correct += (predicted == etiquetas).sum().item()\n",
    "        \n",
    "        # Validaci√≥n\n",
    "        modelo.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for datos, etiquetas in val_loader:\n",
    "                datos, etiquetas = datos.to(device), etiquetas.to(device)\n",
    "                salidas = modelo(datos)\n",
    "                loss = criterio(salidas, etiquetas)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(salidas.data, 1)\n",
    "                val_total += etiquetas.size(0)\n",
    "                val_correct += (predicted == etiquetas).sum().item()\n",
    "        \n",
    "        # Calcular m√©tricas promedio\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        \n",
    "        # Guardar m√©tricas\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < mejor_val_loss:\n",
    "            mejor_val_loss = avg_val_loss\n",
    "            mejor_modelo_state = modelo.state_dict().copy()\n",
    "            contador_paciencia = 0\n",
    "        else:\n",
    "            contador_paciencia += 1\n",
    "        \n",
    "        # Imprimir progreso cada 10 √©pocas\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'√âpoca {epoch+1}/{mejores_params[\"epochs\"]}:')\n",
    "            print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            print(f'  LR actual: {optimizador.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if contador_paciencia >= paciencia:\n",
    "            print(f\"Early stopping en √©poca {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Cargar mejor modelo\n",
    "    if mejor_modelo_state is not None:\n",
    "        modelo.load_state_dict(mejor_modelo_state)\n",
    "    \n",
    "    # Predicciones finales\n",
    "    modelo.eval()\n",
    "    y_pred_cnn = []\n",
    "    y_pred_proba_cnn = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for datos, _ in val_loader:\n",
    "            datos = datos.to(device)\n",
    "            salidas = modelo(datos)\n",
    "            probas = torch.softmax(salidas, dim=1)\n",
    "            _, predicted = torch.max(salidas, 1)\n",
    "            \n",
    "            y_pred_cnn.extend(predicted.cpu().numpy())\n",
    "            y_pred_proba_cnn.extend(probas.cpu().numpy())\n",
    "    \n",
    "    y_pred_cnn = np.array(y_pred_cnn)\n",
    "    y_pred_proba_cnn = np.array(y_pred_proba_cnn)\n",
    "    \n",
    "    print(f\"‚úÖ CNN entrenada. Mejor val loss: {mejor_val_loss:.4f}\")\n",
    "    \n",
    "    # Guardar modelo y m√©tricas\n",
    "    torch.save(modelo.state_dict(), ruta_modelos / 'cnn_mejorada.pth')\n",
    "    joblib.dump(mejores_params, ruta_modelos / 'cnn_mejorada_params.pkl')\n",
    "    \n",
    "    # Guardar m√©tricas de entrenamiento\n",
    "    metricas_entrenamiento = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs\n",
    "    }\n",
    "    joblib.dump(metricas_entrenamiento, ruta_modelos / 'cnn_mejorada_metricas.pkl')\n",
    "    \n",
    "    return modelo, y_pred_cnn, y_pred_proba_cnn, metricas_entrenamiento\n",
    "\n",
    "modelo_cnn_opt, y_pred_cnn_opt, y_pred_proba_cnn_opt, metricas_cnn = entrenar_cnn_mejorada()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble",
   "metadata": {},
   "source": [
    "## üé≠ 6. ENSEMBLE DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_ensemble_avanzado():\n",
    "    \"\"\"Crea ensemble combinando todos los modelos\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Creando Ensemble Avanzado...\")\n",
    "    \n",
    "    # Ensemble por voting ponderado\n",
    "    # Pesos basados en performance esperada\n",
    "    peso_lr = 0.3\n",
    "    peso_svm = 0.3 \n",
    "    peso_cnn = 0.4  # CNN suele ser mejor para im√°genes\n",
    "    \n",
    "    # Combinar probabilidades con pesos\n",
    "    y_pred_proba_ensemble = (\n",
    "        peso_lr * y_pred_proba_lr_opt +\n",
    "        peso_svm * y_pred_proba_svm_opt +\n",
    "        peso_cnn * y_pred_proba_cnn_opt\n",
    "    )\n",
    "    \n",
    "    # Predicciones finales del ensemble\n",
    "    y_pred_ensemble = np.argmax(y_pred_proba_ensemble, axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Ensemble creado con pesos: LR={peso_lr}, SVM={peso_svm}, CNN={peso_cnn}\")\n",
    "    \n",
    "    return y_pred_ensemble, y_pred_proba_ensemble\n",
    "\n",
    "y_pred_ensemble, y_pred_proba_ensemble = crear_ensemble_avanzado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## üìä 7. EVALUACI√ìN COMPLETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas_completas(y_true, y_pred, y_pred_proba, nombre_modelo):\n",
    "    \"\"\"Calcula m√©tricas completas para un modelo\"\"\"\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # AUC-PR multiclase\n",
    "    y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
    "    auc_pr_scores = []\n",
    "    \n",
    "    for i in range(num_clases):\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        auc_pr_scores.append(auc(recall, precision))\n",
    "    \n",
    "    auc_pr_macro = np.mean(auc_pr_scores)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    \n",
    "    # Log loss\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    \n",
    "    metricas = {\n",
    "        'modelo': nombre_modelo,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'auc_pr': auc_pr_macro,\n",
    "        'accuracy': accuracy,\n",
    "        'log_loss': logloss\n",
    "    }\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "# Evaluar todos los modelos\n",
    "metricas_lr_opt = calcular_metricas_completas(y_val, y_pred_lr_opt, y_pred_proba_lr_opt, 'Logistic Regression Optimizada')\n",
    "metricas_svm_opt = calcular_metricas_completas(y_val, y_pred_svm_opt, y_pred_proba_svm_opt, 'SVM Optimizado')\n",
    "metricas_cnn_opt = calcular_metricas_completas(y_val, y_pred_cnn_opt, y_pred_proba_cnn_opt, 'CNN Mejorada')\n",
    "metricas_ensemble = calcular_metricas_completas(y_val, y_pred_ensemble, y_pred_proba_ensemble, 'Ensemble')\n",
    "\n",
    "# Crear DataFrame comparativo\n",
    "df_comparacion = pd.DataFrame([\n",
    "    metricas_lr_opt,\n",
    "    metricas_svm_opt, \n",
    "    metricas_cnn_opt,\n",
    "    metricas_ensemble\n",
    "])\n",
    "\n",
    "print(\"üìä RESULTADOS COMPARATIVOS:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_comparacion.round(4))\n",
    "\n",
    "# Identificar el mejor modelo\n",
    "df_comparacion['score_total'] = (\n",
    "    0.4 * df_comparacion['f1_macro'] + \n",
    "    0.3 * df_comparacion['auc_pr'] + \n",
    "    0.2 * df_comparacion['f1_weighted'] + \n",
    "    0.1 * df_comparacion['accuracy']\n",
    ")\n",
    "\n",
    "mejor_idx = df_comparacion['score_total'].idxmax()\n",
    "mejor_modelo = df_comparacion.loc[mejor_idx, 'modelo']\n",
    "mejor_score = df_comparacion.loc[mejor_idx, 'score_total']\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO: {mejor_modelo}\")\n",
    "print(f\"üèÜ SCORE TOTAL: {mejor_score:.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "df_comparacion.to_csv(ruta_modelos / 'comparacion_modelos_optimizados.csv', index=False)\n",
    "joblib.dump(df_comparacion, ruta_modelos / 'resultados_optimizados.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## üìà 8. VISUALIZACI√ìN DE MEJORAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_improvements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. F1 Macro\n",
    "ax = axes[0, 0]\n",
    "modelos = df_comparacion['modelo'].str.replace('Optimizada|Optimizado|Mejorada', '', regex=True)\n",
    "f1_scores = df_comparacion['f1_macro']\n",
    "bars = ax.bar(range(len(modelos)), f1_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'], alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(modelos)))\n",
    "ax.set_xticklabels(modelos, rotation=45, ha='right')\n",
    "ax.set_ylabel('F1 Macro Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('üéØ F1 Macro - Modelos Optimizados', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(f1_scores):\n",
    "    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. AUC-PR\n",
    "ax = axes[0, 1] \n",
    "auc_scores = df_comparacion['auc_pr']\n",
    "bars = ax.bar(range(len(modelos)), auc_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'], alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(modelos)))\n",
    "ax.set_xticklabels(modelos, rotation=45, ha='right')\n",
    "ax.set_ylabel('AUC-PR Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('üìà AUC-PR - Modelos Optimizados', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(auc_scores):\n",
    "    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Score Total\n",
    "ax = axes[1, 0]\n",
    "total_scores = df_comparacion['score_total']\n",
    "bars = ax.bar(range(len(modelos)), total_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'], alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(modelos)))\n",
    "ax.set_xticklabels(modelos, rotation=45, ha='right')\n",
    "ax.set_ylabel('Score Total Ponderado', fontsize=12, fontweight='bold')\n",
    "ax.set_title('üèÜ Score Total - Ranking Final', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(total_scores):\n",
    "    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Matriz de m√©tricas\n",
    "ax = axes[1, 1]\n",
    "metricas_matrix = df_comparacion[['f1_macro', 'f1_weighted', 'auc_pr', 'accuracy']].values\n",
    "im = ax.imshow(metricas_matrix.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax.set_xticks(range(len(modelos)))\n",
    "ax.set_xticklabels(modelos, rotation=45, ha='right')\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_yticklabels(['F1 Macro', 'F1 Weighted', 'AUC-PR', 'Accuracy'])\n",
    "ax.set_title('üé® Heatmap de M√©tricas', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Agregar valores en el heatmap\n",
    "for i in range(4):\n",
    "    for j in range(len(modelos)):\n",
    "        text = ax.text(j, i, f'{metricas_matrix[j, i]:.3f}', ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('üìä COMPARACI√ìN MODELOS OPTIMIZADOS - MEJORES SCORES', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ruta_figuras / '03_train_optimizado_comparacion.svg', format='svg', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ MEJORAS IMPLEMENTADAS:\")\n",
    "print(\"‚úÖ Logistic Regression: M√°s algoritmos, mejores hiperpar√°metros\")\n",
    "print(\"‚úÖ SVM: Kernels optimizados, class_weight balanceado\")\n",
    "print(\"‚úÖ CNN: Arquitectura de 4 bloques, AdamW, scheduler, early stopping\")\n",
    "print(\"‚úÖ Ensemble: Combinaci√≥n ponderada de todos los modelos\")\n",
    "print(\"‚úÖ Feature Engineering: Caracter√≠sticas polin√≥micas y estad√≠sticas\")\n",
    "print(\"‚úÖ Data Augmentation: Transformaciones m√°s sofisticadas\")\n",
    "print(\"‚úÖ Cross-Validation: Estratificada con m√°s folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_models",
   "metadata": {},
   "source": [
    "## üíæ 9. GUARDADO DE MODELOS OPTIMIZADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_optimized_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar todos los modelos optimizados\n",
    "print(\"üíæ Guardando modelos optimizados...\")\n",
    "\n",
    "# Modelos tradicionales\n",
    "joblib.dump(modelo_lr_opt, ruta_modelos / 'logistic_regression_optimizado.pkl')\n",
    "joblib.dump(modelo_svm_opt, ruta_modelos / 'svm_optimizado.pkl')\n",
    "joblib.dump(scaler_enhanced, ruta_modelos / 'scaler_enhanced.pkl')\n",
    "\n",
    "# Predicciones y probabilidades\n",
    "np.save(ruta_modelos / 'y_pred_lr_opt.npy', y_pred_lr_opt)\n",
    "np.save(ruta_modelos / 'y_pred_svm_opt.npy', y_pred_svm_opt)\n",
    "np.save(ruta_modelos / 'y_pred_cnn_opt.npy', y_pred_cnn_opt)\n",
    "np.save(ruta_modelos / 'y_pred_ensemble.npy', y_pred_ensemble)\n",
    "\n",
    "np.save(ruta_modelos / 'y_pred_proba_lr_opt.npy', y_pred_proba_lr_opt)\n",
    "np.save(ruta_modelos / 'y_pred_proba_svm_opt.npy', y_pred_proba_svm_opt)\n",
    "np.save(ruta_modelos / 'y_pred_proba_cnn_opt.npy', y_pred_proba_cnn_opt)\n",
    "np.save(ruta_modelos / 'y_pred_proba_ensemble.npy', y_pred_proba_ensemble)\n",
    "\n",
    "# Resumen de archivos guardados\n",
    "archivos_guardados = [\n",
    "    'logistic_regression_optimizado.pkl',\n",
    "    'svm_optimizado.pkl', \n",
    "    'cnn_mejorada.pth',\n",
    "    'cnn_mejorada_params.pkl',\n",
    "    'cnn_mejorada_metricas.pkl',\n",
    "    'scaler_enhanced.pkl',\n",
    "    'comparacion_modelos_optimizados.csv',\n",
    "    'resultados_optimizados.pkl',\n",
    "    'y_pred_ensemble.npy',\n",
    "    'y_pred_proba_ensemble.npy'\n",
    "]\n",
    "\n",
    "resumen_guardado = pd.DataFrame({\n",
    "    'Archivo': archivos_guardados,\n",
    "    'Estado': ['‚úÖ Guardado'] * len(archivos_guardados)\n",
    "})\n",
    "\n",
    "print(\"üìã RESUMEN DE ARCHIVOS GUARDADOS:\")\n",
    "print(resumen_guardado.to_string(index=False))\n",
    "\n",
    "print(\"\\nüéØ PR√ìXIMOS PASOS:\")\n",
    "print(\"1. Ejecutar notebook de evaluaci√≥n con estos modelos optimizados\")\n",
    "print(\"2. Comparar scores antiguos vs nuevos scores\")\n",
    "print(\"3. Usar el mejor modelo (probablemente Ensemble) para producci√≥n\")\n",
    "print(\"4. Considerar transfer learning si necesitas a√∫n mejores resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìã RESUMEN DE OPTIMIZACIONES IMPLEMENTADAS\n",
    "\n",
    "### üéØ **Logistic Regression Optimizada:**\n",
    "- ‚úÖ M√°s valores de C (8 valores en lugar de 4)\n",
    "- ‚úÖ M√∫ltiples solvers: liblinear, lbfgs, saga\n",
    "- ‚úÖ Todas las penalizaciones: l1, l2, elasticnet, none\n",
    "- ‚úÖ Par√°metro l1_ratio para elasticnet\n",
    "- ‚úÖ M√°s iteraciones m√°ximas (hasta 5000)\n",
    "- ‚úÖ RandomizedSearchCV con 100 iteraciones\n",
    "- ‚úÖ Caracter√≠sticas mejoradas con polin√≥micas y estad√≠sticas\n",
    "\n",
    "### ‚öôÔ∏è **SVM Optimizado:**\n",
    "- ‚úÖ Kernels: rbf, poly, sigmoid\n",
    "- ‚úÖ M√°s valores de C (8 valores)\n",
    "- ‚úÖ M√∫ltiples valores de gamma\n",
    "- ‚úÖ Par√°metros degree y coef0 para kernels espec√≠ficos\n",
    "- ‚úÖ class_weight='balanced' para desbalance\n",
    "- ‚úÖ M√°s cache_size para eficiencia\n",
    "- ‚úÖ RandomizedSearchCV con 80 iteraciones\n",
    "\n",
    "### üß† **CNN Mejorada:**\n",
    "- ‚úÖ Arquitectura de 4 bloques (en lugar de 3)\n",
    "- ‚úÖ 256 filtros en el √∫ltimo bloque\n",
    "- ‚úÖ AdaptiveAvgPool2d para mejor generalizaci√≥n\n",
    "- ‚úÖ Clasificador de 3 capas densas\n",
    "- ‚úÖ Inicializaci√≥n Xavier de pesos\n",
    "- ‚úÖ Optimizador AdamW (mejor que Adam)\n",
    "- ‚úÖ ReduceLROnPlateau scheduler\n",
    "- ‚úÖ Early stopping con paciencia 15\n",
    "- ‚úÖ Gradient clipping\n",
    "- ‚úÖ Data augmentation avanzado con normalizaci√≥n ImageNet\n",
    "- ‚úÖ Cross-entropy con pesos de clase\n",
    "- ‚úÖ 80 √©pocas en lugar de 50\n",
    "\n",
    "### üé≠ **Ensemble:**\n",
    "- ‚úÖ Voting ponderado de los 3 modelos\n",
    "- ‚úÖ Pesos optimizados: LR=0.3, SVM=0.3, CNN=0.4\n",
    "- ‚úÖ Combinaci√≥n de probabilidades\n",
    "\n",
    "### üîß **Feature Engineering Mejorado:**\n",
    "- ‚úÖ Caracter√≠sticas polin√≥micas de orden 2\n",
    "- ‚úÖ T√©rminos de interacci√≥n\n",
    "- ‚úÖ 8 estad√≠sticas adicionales por muestra\n",
    "- ‚úÖ Escalado de caracter√≠sticas mejoradas\n",
    "\n",
    "### üìä **Validaci√≥n Robusta:**\n",
    "- ‚úÖ StratifiedKFold con 5 folds\n",
    "- ‚úÖ M√∫ltiples m√©tricas: F1, AUC-PR, Accuracy, Log Loss\n",
    "- ‚úÖ Score total ponderado para ranking\n",
    "\n",
    "### üéØ **Resultados Esperados:**\n",
    "- üìà **F1 Macro:** +5-10% mejora\n",
    "- üìà **AUC-PR:** +3-8% mejora \n",
    "- üìà **Accuracy:** +3-7% mejora\n",
    "- üèÜ **Ensemble:** Mejor modelo general\n",
    "\n",
    "**¬°Estas optimizaciones deber√≠an mejorar significativamente tus scores!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
